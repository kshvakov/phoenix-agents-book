---
title: "Глава 6: Эксплуатация и инциденты"
description: "Как сделать инциденты воспроизводимыми: SLI/SLO, triage, сценарии реагирования, условия эскалации и проверяемые runbooks."
weight: 6
---

# Глава 6: Эксплуатация и инциденты (сценарии реагирования + SLI/SLO + triage)

## Пролог: Parts Unlimited, 2026. Brent как единая точка отказа

**Ночь, пятница.** Телефон Brent звонит.

"High CPU на production-сервере `api-gateway-03`. Сервис деградирует. Клиенты затронуты."

Brent просыпается. Открывает ноутбук. Подключается к VPN. Смотрит метрики.

**Его процесс (в голове):**

1. Проверка CPU: `top` показывает, что один процесс забирает почти весь CPU
2. Проверка логов: `tail -f /var/log/api-gateway.log` — видит повторяющуюся ошибку
3. Гипотеза: утечка памяти в процессе X
4. Действие: перезапуск процесса X
5. Проверка: CPU вернулся к норме, сервис восстановился

**Время:** быстро. **Результат:** инцидент решён.

**Проблема:** В 2014 Parts Unlimited это происходило **каждую неделю**:
- Типовые инциденты: высокий CPU, утечка памяти, заполненный диск, исчерпанный пул подключений
- Каждый раз Brent тушит вручную
- Каждый раз тот же процесс, но он в голове Brent
- Никто кроме Brent не может это сделать (Bill Palmer пробовал 2 раза → сделал хуже)

Bill Palmer спросил Brent: "Можешь написать сценарий реагирования? Чтобы команда могла делать рутинные инциденты без тебя?"

Brent ответил: "Пробовал. Написал 10 страниц. Но каждый инцидент немного разный. Команда не понимает когда какой шаг применять. В итоге всё равно звонят мне."

**Корневая причина:**

runbooks в 2014 — это статичный документ. Инцидент — это динамичный процесс. Нужны:
- **Условия:** когда применять шаг A, а когда шаг B
- **Проверка:** как проверить, что шаг сработал
- **Эскалация:** когда звать Brent (если стандартный сценарий не помог)

В 2014 это было в голове Brent. Команда не могла воспроизвести.

---

## Та же задача в 2026: как Lance Bishop решает с агентом

**Контекст:** Lance Bishop в Parts Unlimited 2026 столкнулся с той же проблемой — типовые инциденты тушатся старшим экспертом‑узким местом (Richard Hendricks) вручную. Каждую неделю то же самое: высокий CPU → Richard чинит → быстро → решено.

**Цель:** сделать агента первичным реагирующим для рутинных инцидентов. Richard должен вмешиваться только если агент не справился (эскалация).

**Решение Lance:** создать сценарий реагирования для агента с:
- **Логика триажа:** агент сам определяет тип инцидента
- **Диагностические команды:** агент сам выполняет команды (top, grep, journalctl, systemctl)
- **Проверка:** агент проверяет что исправление сработало
- **Эскалация:** агент зовёт человека если сценарий не помог

**Ключевое отличие:**

В 2014 сценарий реагирования = статичный документ для человека.
В 2026 сценарий реагирования = исполняемый сценарий для агента (агент сам выполняет команды).

---

## Быстрый старт

### Цель

Научить агента обрабатывать типовой инцидент: "High CPU на production-сервере".

### Что нужно

- IDE-агент (например, Cursor Agent) с доступом к метрикам/логам production (read-only)
- Доступ по SSH (read-only для начала)
- 1 типовой инцидент, который senior-инженер решает регулярно

### Промпт runbooks для агента

```
Роль: ты — агент для реагирования на инциденты (первичный реагирующий).

Контекст:
- Алерт: "Высокая загрузка CPU на сервере в продакшене api-gateway-03"
- Порог: CPU > <THRESHOLD> в течение <WINDOW>
- Влияние: сервис деградирует, клиенты затронуты

Задача: выполни сценарий реагирования для инцидента "High CPU"

## RUNBOOKS: High CPU

### Шаг 1: первичный разбор (определи первопричину)

**Команды для выполнения:**

1. Проверь загрузку CPU по процессам:
```bash
ssh api-gateway-03 "top -b -n 1 | head -n <N>"
```

2. Проверь последние логи (последние <N> строк):
```bash
sudo journalctl -u api-gateway --since "<WINDOW> ago" -n <N> | grep -Ei 'error|warn|fatal'
```

3. Проверь список процессов:
```bash
ssh api-gateway-03 "ps aux --sort=-%cpu | head -n <N>"
```

**Анализ:**
- Какой процесс жрёт CPU? (найди по `top`)
- Есть ли ошибки в логах? (найди по `grep`)
- Процесс нормальный (ожидаемый) или аномалия?

**Дерево решений:**

IF процесс ожидаемый (например, `api-gateway`) AND в логах повторяются ошибки THEN:
  → Гипотеза: утечка памяти или бесконечный цикл
  → ПЕРЕЙДИ к шагу 2a (перезапуск процесса)

IF процесс неожиданный (например, `cryptominer`) THEN:
  → Гипотеза: компрометация/взлом
  → ЭСКАЛИРУЙ в команду безопасности (STOP, ничего не меняй)

IF процесс ожидаемый AND ошибок в логах нет THEN:
  → Гипотеза: легитимная высокая нагрузка
  → ПЕРЕЙДИ к шагу 2b (горизонтальное масштабирование)

### Шаг 2a: исправление (перезапуск процесса)

**Предусловие:** процесс ожидаемый, в логах есть ошибки

**Команды:**

1. Graceful restart:
```bash
ssh api-gateway-03 "sudo systemctl restart api-gateway"
```

2. Дождись перезапуска (максимум <ROLLOUT_TIMEOUT>):
```bash
ssh api-gateway-03 "sudo systemctl is-active --quiet api-gateway"  # exit 0 если сервис поднялся
```

**УСЛОВИЯ ОСТАНОВКИ (STOP):**
- Если rollout неуспешен → ЭСКАЛИРУЙ: позови дежурного инженера (STOP)
- Если перезапуск длится > <ROLLOUT_TIMEOUT> → ЭСКАЛИРУЙ (зависло)

### Шаг 2b: исправление (горизонтальное масштабирование)

**Предусловие:** легитимная высокая нагрузка

**Команды:**

1. Проверь текущее число реплик:
```bash
# В systemd-стеке «replicas» = число хостов/инстансов в пуле балансировщика.
# Обычно это хранится в inventory/CMDB. Пример: вывести группу api-gateway из Ansible inventory.
ansible-inventory -i inventories/production --list | jq '.api_gateway.hosts | length'
```

2. Scale up (current + 2):
```bash
# Пример: добавить 2 инстанса в пул и запустить сервис на новых хостах (делает Ansible).
ansible-playbook -i inventories/production playbooks/api-gateway.yml --tags scale --extra-vars "api_gateway_instances=<current+2>"
```

**УСЛОВИЯ ОСТАНОВКИ (STOP):**
- Если достигнут максимум реплик → ЭСКАЛИРУЙ (предел ёмкости)

### Шаг 3: проверка результата

**Команды:**

1. Проверь CPU после исправления:
```bash
ssh api-gateway-03 "top -b -n 1 | head -5"
```

2. Проверь логи (нет новых ошибок):
```bash
sudo journalctl -u api-gateway --since "<WINDOW> ago" -n 50 | grep -i "error"
```

3. Проверь health endpoint:
```bash
curl -s http://api-gateway-03/health | jq '.status'
```

**Критерии успеха:**
- CPU < <THRESHOLD> (держится в течение <WINDOW>)
- Нет ошибок в логах (последние 50 строк)
- Health check возвращает `status: "healthy"`

**Если проверка не прошла:**
- ЭСКАЛИРУЙ: позови дежурного инженера (исправление не сработало)

### Шаг 4: действия после инцидента

**Документация:**

1. Таймлайн инцидента (сгенерируй автоматически):
   - <TIME>: сработал алерт (CPU > <THRESHOLD>)
   - <TIME>: завершён первичный разбор (утечка памяти в api-gateway)
   - <TIME>: начат перезапуск
   - <TIME>: перезапуск завершён
   - <TIME>: проверка прошла (CPU вернулся в норму)

2. Первопричина (на основе логов):
   - Утечка памяти в endpoint `/api/v1/reports`
   - Триггер: запрос с большим датасетом (> <SIZE_THRESHOLD>)

3. Follow-up actions:
   - Создай тикет: «Починить утечку памяти в /api/v1/reports»
   - Добавь мониторинг: алерт, если размер ответа `/reports` > <SIZE_THRESHOLD>

**Формат вывода:**
- JSON-таймлайн
- Уведомление в Slack: «Инцидент решён агентом. Первопричина: утечка памяти. Тикет создан.»

РЕЖИМ ВЫПОЛНЕНИЯ:
- Агент выполняет команды автономно (ssh, journalctl, systemctl, grep, curl)
- Агент показывает результаты каждого шага
- Агент останавливается на эскалации и зовёт дежурного инженера
```

### Шаги

1. **Скопируй промпт** в IDE-агент
2. **Симулируй инцидент** (или дождись реального)
3. **Агент выполняет сценарий** (команды выполняются автономно)
4. **Агент генерирует timeline** + анализ первопричины
5. **Verification:** инцидент решён быстро, без лишней ручной координации

### Пример результата

Агент выполнил сценарий реагирования автономно:

```bash
## Alert fired
Alert: High CPU on api-gateway-03 (CPU выше порога)

# Step 1: Triage (агент выполнил команды)
ssh api-gateway-03 "top -b -n 1 | head -<N>"
... output: api-gateway consumes high CPU ...

sudo journalctl -u api-gateway --since "<WINDOW> ago" -n <N> | grep -Ei 'error|warn|fatal'
... output: repeated OutOfMemoryError ...

# Analysis by agent:
# - Process: api-gateway (expected)
# - Logs: repeated OutOfMemoryError
# - Hypothesis: memory leak
# - Decision: GO TO Step 2a (restart)

# Step 2a: Fix (restart)
ssh api-gateway-03 "sudo systemctl restart api-gateway"
api-gateway restarted

ssh api-gateway-03 "sudo systemctl is-active --quiet api-gateway"
api-gateway is active

# Step 3: Verification
ssh api-gateway-03 "top -b -n 1 | head -<N>"
... output: CPU back to normal ...

sudo journalctl -u api-gateway --since "<WINDOW> ago" -n <N> | grep -Ei 'error|warn|fatal'
... output: no new errors ...

curl -s http://api-gateway-03/health | jq '.status'
"healthy"  # ← health check OK

# Verification: PASSED
# CPU: back to normal
# Logs: no new errors
# Health: healthy
```

**Timeline (сгенерирован агентом):**

```json
{
  "incident_id": "<INCIDENT_ID>",
  "service": "api-gateway",
  "timeline": [
    {"time": "<TIME>", "event": "alert_fired", "details": "CPU > <THRESHOLD>"},
    {"time": "<TIME>", "event": "triage_started", "agent": true},
    {"time": "<TIME>", "event": "triage_completed", "root_cause": "memory_leak", "hypothesis": "OutOfMemoryError in /api/v1/reports"},
    {"time": "<TIME>", "event": "fix_started", "action": "restart deployment"},
    {"time": "<TIME>", "event": "fix_completed", "rollout": "successful"},
    {"time": "<TIME>", "event": "verification_passed", "cpu": "<VALUE>", "status": "healthy"}
  ],
  "resolution_time": "<DURATION>",
  "resolved_by": "agent",
  "escalated": false,
  "follow_up": {
    "ticket": "<TICKET_ID>",
    "title": "Fix memory leak in /api/v1/reports endpoint"
  }
}
```

**Время выполнения:**

- В 2014 Brent вручную: долго (пробуждение, VPN, triage, fix, verify)
- В 2026 агент автономно: быстрее за счёт исполняемого сценария, параллельных проверок и меньшей ручной координации

**Ключевое:** Brent больше не просыпается ночью. Агент решает рутинный инцидент, создаёт тикет для исправления. Brent работает над тикетом в рабочее время.

---

## Теория: сценарии реагирования, SLI/SLO, triage-playbook для агентов

### Концепция 1: Сценарий реагирования для агента и сценарий для человека

**Runbook для человека (2014):**

```markdown
## Инцидент: высокий CPU

1. Check CPU usage: `top`
2. If CPU > <THRESHOLD>, check logs
3. Look for errors
4. If error found, restart service
5. Verify CPU dropped
```

**Проблема:**

- «Проверь логи» — как? Где? Какие ключевые слова искать?
- «Ищи ошибки» — что считается ошибкой?
- «Перезапусти сервис» — мягко или принудительно? `systemctl restart` или полный rollback?
- «Проверь результат» — достаточно ли CPU < 50%? Или нужны другие проверки?

Человек понимает из контекста. Агент — нет.

**Runbook для агента (2026):**

```markdown
## Инцидент: высокий CPU

### Шаг 1: триаж
**Commands:**
- `ssh {{host}} "top -b -n 1 | head -20"`
- `sudo journalctl -u {{service}} --since "<WINDOW> ago" -n 100 | grep -E "(ERROR|FATAL|OutOfMemory)"`

**Дерево решений:**
IF logs contain "OutOfMemoryError" THEN memory_leak ELSE legitimate_load

### Шаг 2: исправление
IF memory_leak:
  - `ssh {{host}} "sudo systemctl restart {{service}}"`
  - WAIT for `ssh {{host}} "sudo systemctl is-active --quiet {{service}}"` (max <WINDOW>)
ELSE IF legitimate_load:
  - увеличить число инстансов через Ansible (добавить хосты в пул + старт `{{service}}` на них)

### Шаг 3: проверка
**Success criteria:**
- CPU < 50% (check with `top`)
- No errors in logs (check last 50 lines)
- Health check returns 200 (check with `curl`)

IF verification failed THEN ESCALATE
```

**Разница:**

- Явные команды (агент выполняет их автономно)
- Явная логика принятия решений (IF/THEN/ELSE, а не «смотря по ситуации»)
- Явные критерии успеха (а не «кажется, стало лучше»)

В 2014 Brent держал это в голове. В 2026 сценарий исполняемый: агент выполняет команды как скрипт.

### Концепция 2: SLI/SLO для измерения качества работы агента

**Проблема в 2014:**

Bill Palmer спросил Brent: "Насколько эффективно ты решаешь инциденты?"

Brent ответил: "Не знаю. Кажется, быстро. Клиенты особо не жалуются."

**Нет метрик** → нет способа улучшить процесс.

**Решение в 2026: SLI/SLO для агента**

**SLI (Service Level Indicator)** — что измеряем:
- **MTTD (Mean Time To Detect):** время от инцидента → алерта → запуска агента
- **MTTR (Mean Time To Resolve):** время от старта агента → проверка прошла
- **Доля эскалаций:** доля инцидентов, где агент эскалировал человеку
- **Доля ложных срабатываний:** % инцидентов, где агент сделал неверное исправление

**SLO (Service Level Objective)** — что ожидаем:
- MTTD в пределах <WINDOW> (алерт должен быстро дойти до агента)
- MTTR в пределах <WINDOW> (агент должен быстро решить рутинный инцидент)
- Доля эскалаций в пределах <THRESHOLD> (агент справляется с большей частью рутины)
- Доля ложных срабатываний ниже <THRESHOLD> (ошибки агента редки и заметны)

**Как измерять:**

Агент логирует каждый инцидент:

```json
{
  "incident_id": "INC-001",
  "detected_at": "<TIME>",
  "agent_started_at": "<TIME>",
  "resolved_at": "<TIME>",
  "escalated": false,
  "fix_correct": true,
  "mttd_seconds": "<SECONDS>",
  "mttr_seconds": "<SECONDS>"
}
```

Dashboard показывает:
- Средний MTTD: в пределах окна
- Средний MTTR: в пределах окна
- Доля эскалаций: ниже порога
- Доля ложных срабатываний: ниже порога

**Цикл улучшений:**

Если SLO нарушен (например, доля эскалаций резко выросла):
- Анализ: на каких инцидентах эскалируем чаще всего?
- Исправление: добавить сценарий реагирования для этого типа инцидента
- Измерь: доля эскалаций вернулась в норму

В 2014 Brent не мог улучшить процесс (нет метрик). В 2026 SLI/SLO делают улучшения измеримыми.

### Концепция 3: плейбук триажа — агент как диагност

**Проблема в 2014:**

Типовой инцидент: «Сервис недоступен».

Brent: «Так, что могло сломаться? БД? Сеть? Память? Диск? Ошибка деплоя?»

Процесс триажа — в голове у Brent. Он проверяет гипотезы одну за другой:
1. Проверить подключение к БД → OK
2. Проверить сеть → OK
3. Проверить память → **высокое потребление памяти** → найдена первопричина

Время: зависит от порядка гипотез и качества сигналов.

**Решение в 2026: Triage-playbook для агента**

```markdown
## Плейбук триажа: сервис недоступен

### Гипотеза 1: нет подключения к БД (вероятность: ВЫСОКАЯ)

**Диагностические команды:**
```bash
ssh {{db_host}} "sudo -u postgres psql -c \"SELECT 1\""
```

**Критерий успеха:** команда возвращает `1`

IF failed:
  - Первопричина: нет подключения к БД
  - Исправление: runbooks «Восстановление подключения к БД»
  - STOP (триаж завершён)

IF passed:
  - Перейти к гипотезе 2

### Гипотеза 2: проблемы сетевой связности (вероятность: СРЕДНЯЯ)

**Диагностические команды:**
```bash
ssh {{host}} "curl -s http://dependency-service/health"
```

**Критерий успеха:** возвращает HTTP 200

IF failed:
  - Первопричина: сеть до dependency-service
  - Исправление: runbooks «Диагностика сети»
  - STOP (триаж завершён)

IF passed:
  - Перейти к гипотезе 3

### Гипотеза 3: исчерпание памяти (вероятность: СРЕДНЯЯ)

**Диагностические команды:**
```bash
ssh {{host}} "free -m"
```

**Критерий успеха:** доступная память > <MEM_FREE_THRESHOLD>

IF failed:
  - Первопричина: OOM (out-of-memory)
  - Исправление: runbooks «Утечка памяти»
  - STOP (триаж завершён)

IF passed:
  - Перейти к гипотезе 4

### Гипотеза 4: диск заполнен (вероятность: НИЗКАЯ)

(аналогично)

### Гипотеза 5: неизвестно (эскалация)

IF все гипотезы проверены AND первопричина не найдена THEN:
  - ESCALATE: позови дежурного инженера
  - Передай: все результаты диагностики + таймлайн
```

**Ключевое:**

- Гипотезы ранжированы по вероятности (проверяем HIGH в первую очередь)
- Каждая гипотеза: явная команда + критерий успеха
- Агент выполняет команды автономно
- Если первопричина найдена → STOP (триаж завершён), GO TO fix

В 2014 Brent делал это «на интуиции» (опыт). В 2026 агент следует плейбуку, который Brent зафиксировал один раз.

**Время триажа:**

- В 2014 Brent: долго (проверяет гипотезы последовательно, порядок зависит от интуиции)
- В 2026 агент: быстрее (проверяет гипотезы в более оптимальном порядке, команды выполняются автономно)

### Концепция 4: эскалация — когда агент зовёт человека

**Зачем нужна эскалация:**

Агент не всемогущ. Есть инциденты которые агент не может решить:
- Новый тип инцидента (нет runbooks)
- Неоднозначная первопричина (несколько гипотез одинаково вероятны)
- Инцидент безопасности (требует оценки человека)
- Исправление не сработало (проверка не прошла)

**Критерии эскалации:**

```markdown
## ESCALATE если:

1. **Неизвестный тип инцидента:**
   - Плейбук триажа проверил все гипотезы
   - Первопричина не найдена
   - → ESCALATE с результатами диагностики

2. **Неоднозначная первопричина:**
   - 2+ гипотезы одинаково вероятны (разница вероятности < 20%)
   - Агент не может выбрать
   - → ESCALATE с гипотезами + обоснованием

3. **Инцидент безопасности:**
   - Триаж нашёл неожиданный процесс (например, cryptominer)
   - Возможно, компрометация/взлом
   - → ESCALATE в команду безопасности (НЕ трогай)

4. **Исправление не сработало:**
   - runbooks выполнен
   - Проверка не прошла (критерии успеха не выполнены)
   - → ESCALATE с таймлайном + результатами неуспешной проверки

5. **Инцидент с высоким влиянием:**
   - Клиенты затронуты > <THRESHOLD>
   - Потери выручки > <THRESHOLD> за <WINDOW>
   - → ESCALATE немедленно (дежурный инженер должен знать)
```

**Как делать ESCALATE:**

```json
{
  "incident_id": "INC-002",
  "escalated_at": "<TIME>",
  "escalation_reason": "fix_failed",
  "timeline": [...],
  "diagnostic_results": {
    "hypothesis_1": "DB connection OK",
    "hypothesis_2": "Network OK",
    "hypothesis_3": "Memory: <VALUE> used (high)"
  },
  "attempted_fix": "restart deployment",
  "verification_result": "failed (CPU still high)",
  "human_action_required": "investigate why restart did not reduce memory usage"
}
```

**В 2014:** Brent решал всё сам (Bus factor = 1). Если Brent застрял → никто не помогает.

**В 2026:** агент пробует решить → если не получилось → эскалирует Brent с полным контекстом (таймлайн, диагностика, попытки исправления). Brent получает всю информацию сразу и не тратит время на сбор контекста.

**Метрика:** доля эскалаций — меньшая часть инцидентов уходит человеку; рутина закрывается по сценариям.

### Концепция 5: Документация после инцидента — автоматическая память

**Проблема в 2014:**

Brent решил инцидент ночью. Утром команда спрашивает: "Что случилось ночью?"

Brent из памяти: "Ну, там memory leak был. Я перезапустил. Вроде помогло."
Brent из памяти: "Ну, там утечка памяти была. Я перезапустил. Вроде помогло."

**Нет документации** → следующий раз тот же инцидент → Brent опять тушит вручную.

**Решение в 2026: агент автоматически документирует**

```markdown
## Отчёт по инциденту: <INCIDENT_ID>

### Кратко
- **Сервис:** api-gateway
- **Влияние:** высокий CPU (above threshold), сервис деградирует
- **Кем решено:** агент (без эскалации)
- **Время восстановления:** <DURATION>

### Хронология
| Время | Событие | Детали |
|------|-------|---------|
| <TIME> | Алерт сработал | CPU > <THRESHOLD> sustained <WINDOW> |
| <TIME> | Агент начал первичный разбор | Checked top, logs, process list |
| <TIME> | Первопричина найдена | OutOfMemoryError in /api/v1/reports |
| <TIME> | Исправление запущено | systemctl restart |
| <TIME> | Исправление завершено | Rollout successful |
| <TIME> | Проверка пройдена | CPU back to normal, no errors, health OK |

### Первопричина
Утечка памяти в эндпоинте `/api/v1/reports`, вызванная запросом с большим объёмом данных (> <SIZE_THRESHOLD>).

### Применённое исправление
Перезапуск деплоймента (временное исправление).

### Дальнейшие действия
- **Тикет <TICKET_ID>:** исправить утечку памяти в эндпоинте `/api/v1/reports` (приоритет: высокий)
- **Мониторинг:** добавить алерт, если размер ответа `/reports` > <SIZE_THRESHOLD> (раннее предупреждение)
- **Обновление runbooks:** добавить шаг «проверить размер ответа» в плейбук триажа

### Выводы
- Поиск первопричины: быстро (плейбук триажа сработал)
- Выполнение исправления: быстро (перезапуск помог)
- **Возможность улучшения:** добавить мониторинг размера ответа (позволит обнаружить проблему до всплеска CPU)
```

**Ключевое:**

- Агент генерирует документацию автоматически (меньше ручной работы)
- Хронология точная (временные метки для каждого события)
- Последующие действия явно указаны (тикет создан, мониторинг добавлен)
- Выводы и уроки — для цикла улучшений

В 2014 Brent документировал "в голове" (или не документировал). В 2026 агент создаёт структурированный отчёт, который команда использует для улучшений.

---

## Практика: создание runbooks для 3 типовых инцидентов

### Инцидент 1: High CPU

**Влияние на бизнес:** сервис деградирует, задержка увеличена, клиенты затронуты

**runbooks:**

```markdown
## runbooks: High CPU

### Предусловия
- Алерт: CPU > <THRESHOLD> sustained <WINDOW>
- Сервис: известен (e.g. api-gateway, report-service)

### Шаг 1: первичный разбор

**Commands (агент выполняет автономно):**
```bash
# 1. Check CPU по процессам
ssh {{host}} "top -b -n 1 | head -20"

# 2. Check recent logs
sudo journalctl -u {{service}} --since "<WINDOW> ago" -n 100 | grep -iE "(error|fatal|oom)"

# 3. Check memory usage
ssh {{host}} "free -m"
```

**Decision logic:**

```python
if "OutOfMemoryError" in logs or memory_available < <THRESHOLD>:
    root_cause = "memory_leak"
    go_to_step = "2a"
elif cpu_process == "expected" and no_errors_in_logs:
    root_cause = "legitimate_high_load"
    go_to_step = "2b"
elif cpu_process == "unexpected":
    root_cause = "suspicious_process"
    escalate_to = "security_team"
else:
    root_cause = "unknown"
    escalate_to = "on_call_engineer"
```

### Шаг 2a: исправление утечки памяти (<WINDOW>)

**Pre-condition:** root_cause = "memory_leak"

**Commands:**
```bash
# Graceful restart
ssh {{host}} "sudo systemctl restart {{service}}"

# Wait for service to become active (max <WINDOW>)
ssh {{host}} "sudo systemctl is-active --quiet {{service}}"
```

**STOP CONDITIONS:**
- Если rollout failed → ESCALATE
- Если timeout > <WINDOW> → ESCALATE

### Шаг 2b: горизонтальное масштабирование (<WINDOW>)

**Pre-condition:** root_cause = "legitimate_high_load"

**Commands:**
```bash
# Scale up через Ansible: добавить 2 инстанса в пул балансировщика и поднять сервис на новых хостах.
ansible-playbook -i inventories/production playbooks/{{service}}.yml --tags scale --extra-vars "{{service}}_instances=<current+2>"
```

**STOP CONDITIONS:**
- Если replicas >= max_replicas → ESCALATE (capacity limit)

### Шаг 3: проверка (<WINDOW>)

**Success criteria:**
- CPU < <THRESHOLD> (sustained <WINDOW>)
- No errors in logs (last 50 lines)
- Health check returns 200

**Commands:**
```bash
# Wait <WINDOW>
sleep <SECONDS>

# Check CPU
ssh {{host}} "top -b -n 1 | head -5"

# Check logs
sudo journalctl -u {{service}} --since "<WINDOW> ago" -n 50 | grep -i "error"

# Check health
curl -s http://{{service}}/health | jq '.status'
```

**IF verification failed:**
- ESCALATE: позови дежурного инженера
- Передай: таймлайн, результаты диагностики, попытки исправления

### Шаг 4: действия после инцидента

**Documentation (агент генерирует автоматически):**
- Incident timeline (JSON)
- Анализ первопричины
- Follow-up ticket (if needed)
- Slack notification: "High CPU incident on {{service}} resolved by agent. Root cause: {{root_cause}}. Ticket: {{ticket_id}}."
```

**Время выполнения:** быстро (за счёт сценариев, параллельных проверок и автоматической документации)

### Инцидент 2: Database connection pool exhausted

**Влияние на бизнес:** сервис не может подключиться к БД, запросы падают, клиенты видят ошибки

**Runbook:**

```markdown
## Runbook: DB Connection Pool Exhausted

### Pre-conditions
- Alert: DB connection errors > threshold (e.g. 10 errors/min)
- Logs show "connection pool exhausted" или "too many connections"

### Шаг 1: триаж (<WINDOW>)

**Commands:**
```bash
# 1. Check current DB connections
ssh {{db_host}} "sudo -u postgres psql -c \"SELECT count(*) FROM pg_stat_activity WHERE application_name='{{service}}'\""

# 2. Check DB max connections
ssh {{db_host}} "sudo -u postgres psql -c \"SHOW max_connections\""

# 3. Check service connection pool config
ssh {{host}} "sudo cat /etc/{{service}}/db.yml | grep pool_size"
```

**Decision logic:**
```python
if current_connections >= max_connections * 0.9:
    root_cause = "pool_exhaustion"
elif current_connections < max_connections * 0.5:
    root_cause = "connection_leak"
else:
    root_cause = "unknown"
    escalate_to = "on_call_engineer"
```

### Шаг 2a: устранение исчерпания пула (<WINDOW>)

**Предусловие:** current_connections >= max_connections * 0.9

**Вариант 1:** увеличить max_connections (на стороне БД)
```bash
# НЕ делай автоматически (нужно одобрение DBA)
# ESCALATE к DBA с рекомендацией
```

**Вариант 2:** горизонтально масштабировать сервис (временное исправление)
```bash
# Если проблема вызвана исчерпанием подключений из-за роста инстансов — временно уменьшить число инстансов
# (и/или увеличить лимиты пула). Делается через Ansible.
ansible-playbook -i inventories/production playbooks/{{service}}.yml --tags scale --extra-vars "{{service}}_instances=<current/2>"
```

**STOP CONDITIONS:**
- Если replicas <= 1 → cannot scale down → ESCALATE

### Шаг 2b: устранение утечки подключений (<WINDOW>)

**Pre-condition:** current_connections < max_connections * 0.5 BUT errors persist

**Commands:**
```bash
# Restart service (kills leaked connections)
ssh {{host}} "sudo systemctl restart {{service}}"
ssh {{host}} "sudo systemctl is-active --quiet {{service}}"
```

### Шаг 3: проверка (<WINDOW>)

**Success criteria:**
- DB connection errors < 1/min
- Current connections < max_connections * 0.7
- Health check returns 200

**Commands:**
```bash
sleep 120

# Check connection count
ssh {{db_host}} "sudo -u postgres psql -c \"SELECT count(*) FROM pg_stat_activity WHERE application_name='{{service}}'\""

# Check error rate (from logs)
sudo journalctl -u {{service}} --since "<WINDOW> ago" -n 200 | grep -c "connection pool exhausted"

# Check health
curl -s http://{{service}}/health
```

**IF verification failed:**
- ESCALATE: позови DBA + дежурного инженера

### Шаг 4: действия после инцидента

**Дальнейшие действия:**
- Тикет: «Проверить размер пула подключений для {{service}}»
- Мониторинг: добавить алерт, если connections > max * 0.7 (раннее предупреждение)
```

**Время выполнения:** быстро

### Инцидент 3: Disk full

**Влияние на бизнес:** сервис не может писать логи/данные, падает, клиенты затронуты

**Runbook:**

```markdown
## Runbook: Disk Full

### Pre-conditions
- Alert: Disk usage > 90%
- Service: crashes или cannot write

### Шаг 1: триаж (<WINDOW>)

**Commands:**
```bash
# 1. Check disk usage
ssh {{host}} "df -h"

# 2. Find largest directories
ssh {{host}} "sudo du -sh /* | sort -hr | head -10"

# 3. Check log files size
ssh {{host}} "sudo du -sh /var/log"
```

**Decision logic:**
```python
if "/var/log" > 80% of used space:
    root_cause = "log_rotation_issue"
elif "/tmp" > 80% of used space:
    root_cause = "temp_files_not_cleaned"
else:
    root_cause = "unknown"
    escalate_to = "on_call_engineer"
```

### Шаг 2: исправление (<WINDOW>)

**Option 1:** Clean old logs (safe)
```bash
# Delete logs older than <RETENTION_WINDOW>
ssh {{host}} "sudo find /var/log -name \"*.log\" -mtime +<DAYS> -delete"

# Verify space freed
ssh {{host}} "df -h"
```

**Option 2:** Clean temp files (safe)
```bash
ssh {{host}} "sudo rm -rf /tmp/*"
ssh {{host}} "df -h"
```

**STOP CONDITIONS:**
- Если disk usage всё ещё > 85% → ESCALATE (cleanup не помог)

### Шаг 3: проверка

**Success criteria:**
- Disk usage < 80%
- Service running (not crashed)
- Health check returns 200

### Шаг 4: действия после инцидента

**Follow-up actions:**
- Ticket: "Fix log rotation for {{service}}"
- Monitoring: Add alert if disk usage > 75% (early warning)
```

**Время выполнения:** быстро

---

## Типовые ошибки

### Ошибка 1: Runbook без verification → агент делает хуже

**Сценарий:**

Runbook для "High CPU":
```markdown
Step 1: Restart service
Step 2: Done
```

Агент выполняет → restart → но CPU всё ещё 85% (исправление не сработало).

Агент думает "Done" → закрывает инцидент → **клиенты всё ещё затронуты**.

**Проблема:** Runbook не проверяет что fix сработал.

**Вывод:**

В 2014 Parts Unlimited junior-инженер (не Brent) пытался решить инцидент по runbook: "Restart service". Restart сделал → подумал "готово" → ушёл спать. Утром оказалось что service всё ещё down (restart не помог).

Bill Palmer добавил в runbook: "После fix **проверь** что CPU упал. Если нет — звони Brent."

В 2026 runbook для агента **требует явной проверки результата**:

```markdown
Step 3: Verification

**Success criteria:**
- CPU < 50%
- No errors in logs
- Health check = 200

**IF any criterion failed:**
- ESCALATE: позови дежурного инженера
- Передай: таймлайн, попытки исправления, результаты неуспешной проверки
```

Агент не может «подумать, что готово» — он проверяет по явным критериям.

### Ошибка 2: Агент выполняет опасные команды без подтверждения человека

**Сценарий:**

runbooks для «медленных запросов к БД»:
```markdown
Шаг 1: остановить медленные запросы
Команда: `kill -9 <pid>`
```

Агент выполняет → убивает запрос → но это была **критичная генерация отчёта для CEO** → отчёт потерян → эскалация к CTO.

**Проблема:** Команда опасная (`kill`), агент выполнил без подтверждения человека.

**Вывод:**

В 2014 Wes Davis (не Brent) «быстро починил» инцидент в продакшене: убил процесс, который «жрал CPU». Оказалось, что это был батч‑джоб расчёта зарплаты → данные payroll потеряны → существенный штраф.

Patty McKee после этого ввела guardrail: «НЕ выполнять kill‑команды без подтверждения человека».

В 2026 runbook для агента имеет **эскалацию для опасных команд**:

```markdown
Шаг 1: найти медленные запросы

**IF query duration > <WINDOW>:**
- ESCALATE: "Found long-running query (duration above threshold). Kill it? [yes/no]" (ask on-call engineer)
- Дождаться подтверждения
- IF approved: выполнить `kill -9 <pid>`
- IF not approved: продолжать мониторинг
```

Агент не выполняет опасные команды автоматически — он сначала спрашивает человека.

**Guardrail:** список опасных команд (`kill`, `rm`, `drop`, `delete`) требует подтверждения человека.

### Ошибка 3: Escalation без контекста → человек тратит время на сбор информации

**Сценарий:**

Агент не может решить инцидент → escalate: "High CPU incident. I don't know what to do."

Дежурный инженер (Richard) просыпается → начинает с нуля: проверяет logs, metrics, timeline → тратит время на сбор информации, которую агент уже собрал.

**Проблема:** Escalation без контекста → человек дублирует работу.

**Вывод:**

В 2014 junior-инженер звонил Brent ночью: "Service down, не знаю что делать."

Brent: "А логи смотрел?"
Junior: "Нет."
Brent: "А метрики?"
Junior: "Нет."

Brent тратил время на сбор информации, которую junior должен был собрать до звонка.

В 2026 агент escalate **с полным контекстом**:

```json
{
  "incident_id": "INC-003",
  "escalated_at": "<TIME>",
  "escalation_reason": "fix_failed",
  "timeline": [
    {"time": "<TIME>", "event": "alert_fired"},
    {"time": "<TIME>", "event": "triage_completed", "root_cause": "memory_leak"},
    {"time": "<TIME>", "event": "fix_attempted", "action": "restart"},
    {"time": "<TIME>", "event": "verification_failed", "cpu": "still high"}
  ],
  "diagnostic_results": {
    "cpu": "85%",
    "memory": "90% used",
    "logs": ["OutOfMemoryError repeated 15 times"],
    "process_list": ["api-gateway: 92% CPU"]
  },
  "attempted_fixes": [
    {"action": "restart deployment", "result": "failed (CPU still high)"}
  ],
  "human_action_required": "investigate why restart did not reduce memory usage"
}
```

Человек (Richard) получает всю информацию сразу → сразу начинает разбор (не тратит время на сбор).

**Принцип:** эскалация = передача полного контекста, а не «я не знаю, что делать».

---

## Параллельный трек: эволюция от 2014 к 2026

### Как это было в 2014 (Bill Palmer, payroll incident)

**Сцена из книги (Главы 18-20 "The Phoenix Project"):**

**Ситуация:** Payroll incident в Parts Unlimited. Сотрудники не получили зарплату в пятницу. CEO в ярости, board требует объяснений. Bill Palmer должен найти причину и восстановить систему немедленно.

**Таймлайн инцидента (2014, ручное реагирование):**

В “ручном” режиме инцидент раскладывается на понятные фазы, каждая из которых растягивается из‑за координации и последовательных проверок:
- **Detection:** сигнал приходит через звонки и переадресации, часть времени теряется на “кто вообще должен смотреть”
- **Триаж:** проверки идут последовательно (каждый участник добавляет кусок контекста), много ручной «склейки»
- **Первопричина:** находится поздно, потому что факты собираются медленно
- **Исправление:** изменение готовится и применяется вручную, подтверждения и коммуникация тормозят
- **Восстановление + документация:** восстановление и post‑mortem делаются вручную, таймлайн — из памяти и заметок

**Стоимость:**
- Простой: бизнес‑ущерб от простоя/деградации
- Время команды: сгоревшее время команды на тушение и координацию
- Репутационный ущерб: эскалации, стресс, потеря доверия

**Почему так долго:**
- **Причина 1:** ручное обнаружение (задержки, звонки между командами)
- **Причина 2:** последовательный триаж (Wes → Patty → Brent, каждый проверяет по очереди)
- **Причина 3:** накладные расходы на телефонную координацию (просто сбор информации)
- **Причина 4:** ручная реконструкция таймлайна (после инцидента, по памяти)

**Рефлексия Bill Palmer:**
> "Долгий MTTR для зарплатного инцидента. Это катастрофа. Сотрудники без зарплаты, CEO в ярости.
>
> Мы тушили пожар вручную: звонки, ручной сбор логов, накладные расходы на координацию.
> Если бы мы могли быстрее обнаруживать, быстрее проводить триаж, быстрее координироваться..."

**Результат Bill (конец книги "The Phoenix Project", после runbooks):**
- Создали runbooks для самых частых инцидентов
- Автоматизировали мониторинг (алерты → Slack, а не звонки)
- Результат: MTTR заметно снижен (быстрое обнаружение + документированный процесс)
- Бизнес: заметное снижение потерь от простоев и ускорение восстановления

---

### Как это стало в 2026 (Lance Bishop, incident с агентом)

**Та же проблема (сбой зарплатного батча, исчерпан пул подключений к БД):**

**Таймлайн инцидента (2026, реагирование с агентом):**

С агентом фазы те же, но исполнение другое:
- **Обнаружение:** сигнал попадает в нужный контур автоматически (без ручной переадресации)
- **Триаж:** проверки делаются параллельно, контекст собирается в один пакет
- **Первопричина:** подтверждается быстрее за счёт фактов и корреляции сигналов
- **Исправление:** агент готовит предложение и план отката, человек подтверждает
- **Восстановление + документация:** проверка результата и отчёт формируются автоматически по фактам

**Итоговый MTTR:** заметно меньше за счёт автоматизированного обнаружения, параллельного триажа и минимальной ручной координации.

**Стоимость:**
- Простой: бизнес‑ущерб от простоя/деградации (в этом сценарии существенно меньше)
- Время команды: минимум ручной координации (человек остаётся на подтверждении и ответственности)
- Репутационный ущерб: ниже, потому что восстановление происходит быстрее и предсказуемее

**Почему так быстрее и устойчивее:**
- обнаружение автоматизировано
- триаж параллелизирован
- контекст собирается автоматически (без “склейки” и звонков)
- таймлайн и отчёт формируются из фактов, а не из памяти

**Рефлексия Lance:**
> "Bill Palmer добился улучшения через runbooks и дисциплину процесса.
> Мы добились ещё большего эффекта с агентами.
>
> Это не замена runbooks. Это runbooks **+ исполнение агентом**:
> - Runbook Bill: «1. Проверь логи. 2. Проверь БД. 3. Проверь деплои».
> - Наш runbook: агент выполняет шаги **параллельно** и возвращает контекст быстрее
> - Роль человека: подтвердить исправление и держать ответственность, а не «делать всё вручную»
>
> Агенты усиливают эффект runbooks."

---

### Эффект мультипликатора: агенты усиливают улучшения Phoenix Project

Агенты не "заменяют" runbooks Bill Palmer. Они усиливают эффект:
- обнаружение и первичный разбор становятся более автоматизированными
- проверки и сбор контекста выполняются быстрее и воспроизводимее
- стоимость координации падает, потому что меньше ручной “склейки”

**Что агенты добавляют к runbooks Bill:**
- **Параллельное выполнение:** агент выполняет диагностические шаги параллельно (не последовательно)
- **Меньше координации:** агент строит контекст автоматически (без звонков/Slack‑переписок)
- **Автоматическая хронология:** хронология из логов (без восстановления “по памяти”)
- **Умный первичный разбор:** агент ранжирует гипотезы по вероятности (не “угадывает”)
- **Мгновенная документация:** отчёт по инциденту генерируется автоматически

---

### Эволюция, а не революция

**Ключевая разница:**
- **В 2014:** Bill Palmer создал runbooks (человек читает и исполняет шаги вручную)
- **В 2026:** Lance создал runbooks **+ агент автономно исполняет шаги**

**Не замена, а усиление:**

Вместо сравнения “в минутах” важнее различие в режиме исполнения:
- **В 2014:** человек читает runbook и последовательно выполняет шаги; контекст собирается вручную; отчёт пишется “после”
- **В 2026:** агент исполняет runbook, собирает факты и контекст автоматически; человек подтверждает исправления и держит ответственность; отчёт формируется по фактам в процессе

**Параллели с управлением людьми:**

**Bill Palmer (2014) координировал команду через runbooks:**
- Runbook: «1. Проверь логи. 2. Проверь БД. 3. Если непонятно — звони Brent».
- Wes читает шаг 1 → выполняет
- Patty читает шаг 2 → выполняет
- Brent читает шаг 3 → выполняет
- Итог: последовательное выполнение и ожидания

**Lance (2026) оркеструет агента через runbook:**
- Runbook: те же шаги
- Агент выполняет шаги 1–3 **параллельно**
- Агент показывает результаты + предлагает исправление → человек подтверждает (быстрое ревью)
- Итог: быстрее за счёт параллельности и автоматизированного сбора контекста

**Ключевой навык переносится:**
- Bill документировал процесс реагирования (создание runbook)
- Lance документирует тот же процесс (runbook для агента)
- Принципы те же: триаж → диагностика → исправление → проверка
- Исполнение заметно быстрее (агент не ждёт и выполняет шаги параллельно)

---

### Сравнение Phoenix Project: реагирование Bill на инциденты и подход с агентами

Bill Palmer показал, что дисциплина процесса и runbooks способны перевести реагирование на инциденты из хаоса в управляемость.

Lance с агентами делает следующий шаг: превращает runbooks из документов “для чтения” в исполняемые сценарии — с проверками, ограничениями и эскалацией.

Наблюдаемые отличия:
- меньше ручной координации (контекст собирается автоматически)
- быстрее первичный разбор за счёт параллельных проверок
- ниже зависимость от “единственного эксперта” в рутинных кейсах
- лучше пост-инцидентная память (хронология и отчёт рождаются из доказательств)
- безопаснее выполнение (подтверждение и условия остановки встроены по умолчанию)

---

## Бизнес-эффект: что меняется после автоматизации реагирования на инциденты

### Экономия времени
- меньше ручного triage и “склейки” контекста
- меньше повторяемых действий (проверки и сценарии становятся стандартом)
- меньше эскалаций “вслепую”: человек получает уже собранный контекст

### Улучшения качества
- выше качество диагностики за счёт параллельных проверок и фиксированной процедуры
- лучше документация: отчёт и хронология рождаются из доказательств
- устойчивее дежурства: меньше пожаров, больше предсказуемости и меньше выгорания

---

### Организационный эффект

**Зависимость от эксперта (MTTR без эксперта):**
- До: все инциденты проходят через эксперта (эксперт нужен для первичного разбора, 100% зависимость)
- После: 85% инцидентов решаются агентом автономно (эксперт только для граничных случаев)
- Эффект: эксперт освобождается от рутины и возвращается к стратегической ёмкости

**Масштабируемость команды (покрытие дежурств):**
- До: только 2 эксперта могут разбирать инциденты (Bus factor = 2, ротация дежурств ограничена)
- После: любой инженер + агент могут разбирать рутинные инциденты (Bus factor = 5+, ротация гибче)
- Эффект: нагрузка дежурств распределяется шире, снижается нагрузка на одного человека

**Обучение на инцидентах (непрерывные улучшения):**
- До: разбор инцидентов раз в квартал (качественно, по памяти)
- После: автоматизированный разбор по каждому инциденту (количественно, с фактами из логов)
- Эффект: быстрее выявляются повторяющиеся паттерны → проактивные исправления → меньше будущих инцидентов

---

### Экономическая модель (под ваш контекст)

Экономический эффект здесь считается не “по волшебной формуле из книги”, а по вашей модели стоимости времени и риска.

Чтобы посчитать под ваш контекст:
- зафиксируйте базу (как сейчас тушатся инциденты, где теряется время, где риск)
- опишите целевую схему (что делегируется агенту, где нужно подтверждение, какие контрольные точки)
- оцените эффект по переменным (время инженеров, стоимость простоев/деградаций, стоимость рисков)

Шаблон для расчёта и коммуникации со стейкхолдерами — в [Приложении A]({{< relref "appendix/A-business-case.md" >}}).

---

### Сравнение Phoenix Project

**Bill Palmer (2014, улучшения реагирования на инциденты):**
- Инвестиция: время команды и дисциплина процесса
- Хронология: долго, потому что многое делалось вручную и «с нуля»
- Эффект: инциденты становятся управляемее, но остаются человеко‑центричными
- Зависимость от эксперта: снижается, но runbooks остаются документами для людей

**Lance Bishop (2026 с агентами):**
- Инвестиция: время на runbooks, контрольные точки, проверки и безопасное развёртывание
- Хронология: короче за счёт делегирования повторяемых шагов
- Эффект: быстрее восстановление в рутинных кейсах, меньше хаоса и ручной координации
- Зависимость от эксперта: заметно ниже (эксперт подключается к граничным случаям)

**Мультипликатор агента для реагирования на инциденты:**
- быстрее путь от «документа» к исполняемому сценарию
- ниже цена ручной координации
- меньше зависимости от эксперта в рутинных кейсах

---

### Сводные метрики (главы 1 → 6)

Вместо “накопительных цифр” держим фокус на накопительном эффекте:
- с каждой главой растёт воспроизводимость процесса
- снижается доля ручной рутины и хаоса координации
- качество и безопасность закрепляются через gates, eval и операционные процедуры

---

### Ускорение трансформации бизнеса

После того как runbooks стали исполняемыми сценариями, трансформация ускоряется:
- операционная устойчивость растёт, потому что рутина делегирована и проверяемость встроена
- эксперты перестают быть узким местом и возвращаются к стратегической работе
- появляется замкнутый цикл улучшений: каждый инцидент улучшает сценарии и проверки

---

### Сравнение: агенты и альтернативы для реагирования на инциденты

**Альтернатива 1: нанять SRE-команду (3 человека, покрытие 24/7)**
- Стоимость: найм и онбординг (время и деньги)
- Риски: всё ещё ручное реагирование, человеческие ошибки, выгорание

**Альтернатива 2: купить инструмент управления инцидентами (PagerDuty + автоматизация runbooks)**
- Стоимость: лицензии + интеграции + поддержка
- Риски: привязка к вендору, тяжёлая интеграция, “runbooks как отдельный продукт”

**Альтернатива 3: агент-runbooks (выбрано)**
- Стоимость: время на runbooks, контрольные точки, обучение и безопасное развёртывание
- Реализация: быстрее, если есть шаблоны и дисциплина процесса
- Поддержка: регулярное обновление сценариев и проверок
- Риски: снижены через контрольные точки + условия остановки + подтверждение человека для изменений в продакшене

**Преимущество агентов:**
- меньше ручной координации и меньше вариативности исполнения
- быстрее путь от “документа” к исполняемому сценарию
- без привязки к вендору: runbooks = промпты (переносимо, не проприетарный формат)

---

### Следующие шаги

**Что сделать сразу:**
- Определи типовые инциденты, которые повторяются и отнимают больше всего внимания
- Создай runbooks для одного типового инцидента (обнаружение → первичный разбор → исправление → проверка)
- Добавь плейбук триажа (диагностика по гипотезам, ранжирование по вероятности)
- Протестируй runbooks на стейджинге/в песочнице: смоделируй, проверь, что агент выполняет сценарий корректно

**Что подготовить к следующей главе (Глава 7 — Безопасность и инфраструктура):**
- runbooks готовы → в Главе 7 добавим модель угроз (что может пойти не так)
- Подтверждено, что агент помогает реагировать на инциденты → в Главе 7 применим к изменениям инфраструктуры (безопасные деплои)
- У команды выросла уверенность → в Главе 7 масштабируем на раскатку в продакшен (постепенно, с откатом)

---

## Резюме

### Что мы сделали

Научили агента быть первичным реагирующим для рутинных инцидентов:

1. **runbooks:** исполняемый сценарий (агент выполняет команды автономно)
2. **Плейбук первичного разбора:** диагностика через гипотезы (ранжирование по вероятности)
3. **SLI/SLO:** метрики для измерения качества работы агента (MTTD, MTTR, доля эскалаций)
4. **Эскалация:** когда агент зовёт человека (с полным контекстом)
5. **Документация после инцидента:** автоматическая память (хронология, первопричина, дальнейшие действия)

### Почему это работает

**В 2014:** Brent тушил инциденты вручную. Bus factor = 1. Команда зависела от Brent. Нагрузка на Brent была неустойчивой (выгорание как норма).

**В 2026:** агент тушит рутинные инциденты. Brent вмешивается только при эскалации. Brent возвращается к более устойчивому графику.

**Ключевое отличие:**

- В 2014: знания в голове Brent, процесс не воспроизводим
- В 2026: знания в runbooks + плейбуке первичного разбора, агент выполняет команды автономно

### Метрики успеха

**До runbook + агента (2014):**
- MTTR: долго (Brent вручную)
- Доля эскалаций: почти всегда (всё через Brent)
- Bus factor: 1 (только Brent)
- Нагрузка Brent: высокая

**После runbook + агента (2026):**
- MTTR: быстрее (агент автономно)
- Доля эскалаций: меньшая доля (рутина решается агентом)
- Bus factor: 3+ (агент + любой инженер с runbook)
- Нагрузка Brent: более устойчивая (рутинные инциденты делегированы агенту)

### Критерии приёмки главы

Вы успешно освоили материал, если можете:

**Уровень 1: Понимание**
- Объяснить разницу между runbook для человека и runbook для агента
- Объяснить, что такое SLI/SLO и зачем они нужны
- Перечислить критерии эскалации (когда агент зовёт человека)

**Уровень 2: Применение**
- Создать runbook для 1 типового инцидента в вашем проекте
- Определить SLI/SLO для агента (MTTD, MTTR, доля эскалаций)
- Написать плейбук триажа (диагностика по гипотезам)

**Уровень 3: Воспроизводимость**
- Агент успешно решил рутинный инцидент по runbooks
- Проверка пройдена (критерии успеха выполнены)
- Документация после инцидента сгенерирована автоматически

**Уровень 4: Масштабирование**
- Агент решает 80%+ рутинных инцидентов без эскалации
- SLI/SLO выполняются (MTTD < <WINDOW>, MTTR < <WINDOW>, доля эскалаций < <THRESHOLD>)
- Нагрузка старшего инженера снижена на 50%+ (рутинные инциденты делегированы агенту)

### Следующие шаги

**Глава 7:** Безопасность и инфраструктура — научим безопасно деплоить агента в продакшен (модель угроз + план изменений + план отката).

**Связь с Главой 6:** runbooks помог агенту тушить инциденты, но как безопасно внедрить агента в продакшен? Глава 7 покажет модель угроз и меры снижения рисков.

---

**Артефакты главы:**
-  runbooks для 3 типовых инцидентов (высокая загрузка CPU, исчерпан пул подключений к БД, диск заполнен)
-  Плейбук триажа (диагностика по гипотезам)
-  Определения SLI/SLO (MTTD, MTTR, доля эскалаций, доля ложных срабатываний)
-  Критерии эскалации (когда агент зовёт человека)
-  Шаблон документации после инцидента (хронология, первопричина, дальнейшие действия)
